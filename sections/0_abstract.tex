\twocolumn[
    \begin{center}
        \Large{\uppercase {3RSM}}\\
        \huge\textbf{A Mathematical Introduction to Artificial Neural Networks}\\
        \vspace{0.4cm}
        \normalsize\textbf{Aman}\\
        \vspace{0.1cm}
        \today\\
    \end{center}
]

\begin{abstract}
    This article will provide an overview of the mathematics underlying the operation of supervised feed-forward artificial neural networks, with a particular emphasis on forward and back propagation and a biased collection of common maps and models. The architecture of a neural network is investigated for both a single-layer and two-layer neural network, along with their corresponding matrix calculations. The latter is further developed into error back propagation through the use of well-known calculus to derive formulas. Gradient descent training results in parameter convergence, resulting in an optimal model for a given objective. 
    
    \medskip
    \textbf{\textit{Keywords}} -- 
    Artificial Neural Network,
    Activation Function,
    Single-Layer,
    Perceptron,
    Multi-Layer,
    Forward Propagation,
    Error Back-Propagation,
    Gradient Descent.

\end{abstract}
