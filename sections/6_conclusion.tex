ANNs are a highly effective and surprisingly easy strategy for solving complex problems that are built using well-known mathematics.
This report demonstrated how, without specifying the particular problem that the neural network is charged with solving, the network is capable of forming a function that can map the inputs to the appropriate outputs through a cascade of transformations.
In fact, it is proven that for certain activation functions and a very large number of neurons, ANNs can model any continuous, smooth function arbitrarily well, a result known as the universal approximation theorem \parencite{Goodfellow-et-al-2016}.
This is extremely convenient, since an ANN, like the brain, should theoretically be capable of learning any function given assigned data.
If ANNs could only learn one type of function, the types of problems to which they could be applied would be extremely limited, an issue of data-specific programming. The fact that ANNs can solve different types of problems with relatively the same structure is a desirable advantage.

With the scale of neural networks doubling every 2.4 years according to \textcite{Goodfellow-et-al-2016}, the potential of machine learning to overcome exponentially complex challenges that humanity is actually facing becomes more possible in the future.  We have already seen instances of this, for example the Google DeepMind team, introduced at the beginning of this report, who are responsible for using deep neural networks to produce an outperforming-human bot, have developed AlphaFold. AlphaFold is able to determine the shape that proteins fold into with high accuracy \parencite{dasdas}, known as the protein folding problem, and ``has stood as a grand challenge in biology for the past 50 years'' \parencite{deepmindfold}.
The outcome of decades of research promises an exciting society-benefiting future with the use of ANNs.